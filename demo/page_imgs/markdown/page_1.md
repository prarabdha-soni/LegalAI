LLaMA: Open and Efficient Foundation Language Models Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin Edouard Grave; Guillaume Lample* Meta AI Abstract

We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community $^1$ . 1 Introduction Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examples ( Brown et al. , 2020 ) . These few-shot properties first appeared when scaling models to a sufficient size ( Kaplan et al. , 2020 ) , resulting in a line of work that focuses on further scaling these models ( Chowdhery et al. , 2022 ; Rae et al. , 2021 ) . These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from Hoffmann et al. ( 2022 ) shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. ( 2022 ) is to determine how to best scale the dataset and model sizes for a particular training compute budget. However, this objective disregards the inference budget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of

* $^1$ $^2$ $^3$ $^4$ $^5$ $^{th}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $^{}$^{}$ $^{}$ $

